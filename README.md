# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Problem Statement
In this project a bank marketing [dataset](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv) is used.
It contains phone calls from a direct marketing compaign of a Portoguese banking institution.

The dataset has a series of information (age, job, marital, education, etc...) for a total of 32950 observations, 20 features, and a target variable (y)
with two possible values: yes or no.
The task is addressed as a classification task and the goal is to predict if a client will subscribe a term deposit (y variable).

Two different approaches have been investigated. The first one use a logistic regression model with hyperparameters tuning using HyperDrive,
the second one use the power of AutoML.

The main steps are reported in the diagram below:
![Steps](https://github.com/peppegili/1_Optimizing_an_ML_Pipeline_in_Azure/blob/master/img/problem_statement_steps.png)

The [udacity-project.ipynb](https://github.com/peppegili/1_Optimizing_an_ML_Pipeline_in_Azure/blob/master/udacity-project.ipynb) jupyter notebook contains all the steps of the entire procedure.

## Scikit-learn Pipeline
** Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm. **

In this first task, an HyperDrive pipeline using a scikit-learn logistic regression model is built.
Logistic regression is a classification algorithm used when the dipendent variable (y) is categorical. It uses the logistic function to model the probability of a certain class or event (yes or no).

The [train.py](https://github.com/peppegili/1_Optimizing_an_ML_Pipeline_in_Azure/blob/master/train.py) script contains the following steps:

  - Load data as TabularDataset using TabularDatasetFactory
  - Transform and clean data: the clean_data() function is used for handle missing values, create dummies variables for categorical features, mapping values and other transformations in order to make data suitable for modeling
  - Split data into train and test set: 80% train, 20% test
  - Train the logistic regression model on training data with two hyperparameters:
  
    - *C*: inverse of regularization strength. Smaller values cause stronger regularization
    - *max_iter*: maximum number of iterations for model to converge
    
    The goal is to tune these two parameters using HyperDrive.

Hyperparameter tuning can be computationally expensive, so HyperDrive helps to automate and speeds up hyperparameter tuning process, choosing these parameters.
*HyperDriveConfig* class is responsable of the hyperparameters tuning process. It includes information about hyperparameter space sampling, termination policy, primary metric and estimator.

Specify hyperparameter space sampling and termination policy is very important:

  - Hyperparameter space sampling: ***RandomParameterSampling*** randomly select hyperparameters values over the search space.
    It is not computationally expensive and it is not exhaustive but it works well in most cases. *C* and *max_iter* parameters have been passed to the sampler:
  
    ```
    # Specify parameter sampler
    ps = RandomParameterSampling(
      {
          "--C": uniform(0.1, 1.0),
          "--max_iter": choice(25, 50, 100, 150)
      }
    )
    ```

  - Termination policy: ***BanditPolicy*** defines an early termination policy based on slack criteria, and a frequency and delay interval for evaluation.
    Any run that doesn't fall within the slack factor or slack amount of the evaluation metric with respect to the best performing run will be terminated.
    It automatically terminates poorly performing runs, saving time and improving computational efficiency:
    
    ```
    # Specify a Policy
    policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)
    ```

After submitting the hyperdrive run to the experiment, the best run and the related metrics have been collected:
```
best_run_hdr = hdr.get_best_run_by_primary_metric()
best_run_metrics_hdr = best_run_hdr.get_metrics()
best_params_hdr = best_run_hdr.get_details()['runDefinition']['arguments']

print('Best run ID: ', best_run_hdr.id)
print('Best run Accuracy: ', best_run_metrics_hdr['Accuracy'])
print('Metrics: ', best_run_metrics_hdr)
```
```
Best run ID: xxxxxxxxx
Best run Accuracy: xxxxxxxx
Metrics: xxxxxxx
```
Best model:
  - ***Parameters***: *C* =x , *max_iter* = x
  - ***Accuracy***: xxxx


## AutoML
** Describe the model and parameters generated by AutoML. **

In this second task, an AutoML pipeline is built.
Automated machine learning, also referred to as automated ML or AutoML, is the process of automating the time-consuming, iterative tasks of machine learning model development. It allows to build ML models with high scale, efficiency, and productivity all while sustaining model quality.

*AutoMLConfig* class is responsable of the automated machine learning process. It contains the parameters for configuring the experiment run:

```
# Set parameters for AutoMLConfig
automl_config = AutoMLConfig(
    experiment_timeout_minutes=30,
    task='classification',
    primary_metric='accuracy',
    training_data=train_data,
    label_column_name='y',
    n_cross_validations=4,
    compute_target=compute_cluster)
```

AutoML pipeline is performed following these step:

  - Load data as TabularDataset using TabularDatasetFactory
  - Transform and clean data: the clean_data() function inside *train.py* script is used again
  - Split data into train and test set: 80% train, 20% test
  - Concatenate features and target of training data in order to be correctly used in AutoMLConfig
  - Instantiate AutoMLConfig class with the parameters listed above
  - Submit AutoML run

After submitting the automl run to the experiment, the best run and the related metrics have been collected:
```
best_run_automl, best_model_automl = automl.get_output()
best_run_metrics_automl = best_run_automl.get_metrics()

print('Best run ID: ', best_run_automl.id)
print('Best run Accuracy: ', best_run_metrics_automl['Accuracy'])
print('Metrics: ', best_run_metrics_automl)
```
```
Best run ID: xxxxxxxxx
Best run Accuracy: xxxxxxxx
Metrics: xxxxxxx
```

Best model: ***VotingEnsemble***:
  - ***Accuracy***: xxxxx
  - ***Parameters***:
    - xxxxxx
    - xxxxxx
    - xxxxxx
    - xxxxxx

## Pipeline comparison
** Compare the two models and their performance. **

## Future work
** What are some areas of improvement for future experiments? Why might these improvements improve the model?**

## Proof of cluster clean up
** If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section. **
** Image of cluster marked for deletion **
